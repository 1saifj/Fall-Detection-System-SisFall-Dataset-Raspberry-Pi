{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tf-nightly\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport tensorflow as tf\nimport seaborn as sns\nfrom pylab import rcParams\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom sklearn.preprocessing import OneHotEncoder\nRANDOM_SEED = 42\ncolumns = ['x-axis', 'y-axis', 'z-axis','activity']\ndf = pd.read_csv('../input/sisfall/SISFALL_5CLASS.csv', header = 0, names = columns)\ndf = df.dropna()\ndf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-22T07:26:57.829065Z","iopub.execute_input":"2021-06-22T07:26:57.829451Z","iopub.status.idle":"2021-06-22T07:27:06.760588Z","shell.execute_reply.started":"2021-06-22T07:26:57.829373Z","shell.execute_reply":"2021-06-22T07:27:06.759775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\ndf1=df[['x-axis', 'y-axis', 'z-axis']]\ndf1=df1*(32.0 / 8192.0)\ny = df[['activity']]\ndf1['activity'] = y.values\n\nscale_columns = ['x-axis', 'y-axis', 'z-axis']\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler = scaler.fit(df1[scale_columns])\ndf1.loc[:, scale_columns] = scaler.transform(df1[scale_columns].to_numpy())\nvalues = df1[['x-axis', 'y-axis', 'z-axis']]\nlabels = df1['activity']\n# X['activity'] = y.values\n# values=X[['x-axis', 'y-axis', 'z-axis']]\n# labels=X['activity']\nvalues","metadata":{"execution":{"iopub.status.busy":"2021-06-22T08:01:02.200725Z","iopub.execute_input":"2021-06-22T08:01:02.201059Z","iopub.status.idle":"2021-06-22T08:01:06.811337Z","shell.execute_reply.started":"2021-06-22T08:01:02.201026Z","shell.execute_reply":"2021-06-22T08:01:06.810287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset(X, y, time_steps=1, step=1):\n    Xs, ys = [], []\n    for i in range(0, len(X) - time_steps, step):\n        v = X.iloc[i:(i + time_steps)].values\n        labels = y.iloc[i: i + time_steps]\n        Xs.append(v)        \n        ys.append(stats.mode(labels)[0][0])\n    return np.array(Xs), np.array(ys).reshape(-1, 1)\n\nTIME_STEPS = 200\nSTEP = 40\n##############################  CHECK THISSSSSSSSSSSSSS SAIF!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nXdata, Ylabel = create_dataset(\n    values, \n    labels, \n    TIME_STEPS, \n    STEP)\n\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\nenc = enc.fit(Ylabel)\nYlabel = enc.transform(Ylabel)\n\nX_train, X_test, y_train, y_test = train_test_split(\n        Xdata, Ylabel, test_size=0.2, random_state=RANDOM_SEED,stratify=Ylabel)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T08:01:16.814717Z","iopub.execute_input":"2021-06-22T08:01:16.815043Z","iopub.status.idle":"2021-06-22T08:01:48.551802Z","shell.execute_reply.started":"2021-06-22T08:01:16.815012Z","shell.execute_reply":"2021-06-22T08:01:48.550968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T08:02:05.411133Z","iopub.execute_input":"2021-06-22T08:02:05.411498Z","iopub.status.idle":"2021-06-22T08:02:05.420829Z","shell.execute_reply.started":"2021-06-22T08:02:05.411458Z","shell.execute_reply":"2021-06-22T08:02:05.419971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# model = load_model('../input/lstmfall/saved_model/1')\n# from keras.models import load_model\n# model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n# model.summary()\n# def estimate(modelName,inputSignal):\n#     model = load_model(modelName)\n#     yhat = model.predict(inputSignal)\n    \n#     indices = []\n#     for i in range(yhat.shape[0]):\n#         maxIndex = np.where(yhat[i] == np.amax(yhat[i]))[0]\n#         indices.append(maxIndex)\n        \n#         print('\\nClass of Object Detected for Signal ' + str(i+1) + ' :')\n#         print('Wall = ' + str(round(yhat[i][0]*100,2)) + '%')\n#         print('Human = ' + str(round(yhat[i][1]*100,2)) + '%')\n#         print('Car = ' + str(round(yhat[i][2]*100,2)) + '%')\n\n#     y_pred = np.vstack(indices)\n    \n#     return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score\n# import matplotlib.pyplot as plt\n# y_pred = model.predict(X_test,verbose=1)\n# evaluate predictions\n# acc = accuracy_score(y_test, yhat)\n# print(acc)\n\n# connect predictions with outputs\n# for i in range(2):\n# #     plt.plot(X_train[i])\n# # #     plt.plot((yhat[i]*100).round())\n# #     plt.show()\n# # # \tprint(X_test[i], (yhat[i]*100).round())\n#     print(len(X_train[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade keras","metadata":{"execution":{"iopub.status.busy":"2021-06-21T11:04:03.478062Z","iopub.execute_input":"2021-06-21T11:04:03.47836Z","iopub.status.idle":"2021-06-21T11:04:10.889468Z","shell.execute_reply.started":"2021-06-21T11:04:03.478333Z","shell.execute_reply":"2021-06-21T11:04:10.888528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nimport os\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,LSTM,Dropout,BatchNormalization\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Bidirectional\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\n\nBATCH_SIZE = 128\nmodel = Sequential()\nmodel.add(BatchNormalization())\nmodel.add(LSTM(256, return_sequences=True, input_shape=([X_train.shape[1], X_train.shape[2]])))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(50))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(y_train.shape[1], activation='softmax'))  \n# opt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nhistory = model.fit(X_train, y_train, epochs=220, batch_size=BATCH_SIZE, validation_data=(X_val,y_val),verbose=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:21:49.554379Z","iopub.execute_input":"2021-06-22T09:21:49.554806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"export_dir = 'saved_model/1'\n\ntf.saved_model.save(model, export_dir)\n_, accuracy = model.evaluate(X_test, y_test, verbose=1)\nprint(accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pathlib\n# export_dir = '../input/lstmfall/saved_model/1'\n\n\n# Convert the model\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n#                                       tf.lite.OpsSet.SELECT_TF_OPS]\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model.tflite', 'wb') as f:\n  f.write(tflite_model)\n\n\ntflite_model_file = pathlib.Path('./model.tflite')\ntflite_model_file.write_bytes(tflite_model)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # y_pred=model.predict(X_test)\n# import pathlib\n# export_dir = '../input/lstmfall/saved_model/1'\n\n# mode = \"Speed\" \n\n# if mode == 'Storage':\n#     optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n# elif mode == 'Speed':\n#     optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n# else:\n#     optimization = tf.lite.Optimize.DEFAULT\n\n# # EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter\n\n# converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n# # converter.allow_custom_ops = True\n\n# # Set the optimzations\n# converter.optimizations = [optimization]\n# converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\n# # Invoke the converter to finally generate the TFLite model\n# tflite_model = converter.convert()\n\n# tflite_model_file = pathlib.Path('./model.tflite')\n# tflite_model_file.write_bytes(tflite_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ndef plot_cm(y_true, y_pred, class_names):\n  cm = confusion_matrix(y_true, y_pred)\n  fig, ax = plt.subplots(figsize=(18, 16)) \n  ax = sns.heatmap(\n      cm, \n      annot=True, \n      fmt=\"d\", \n      cmap=sns.color_palette(\"muted\"),\n      ax=ax)\n\n  plt.ylabel('Actual')\n  plt.xlabel('Predicted')\n  ax.set_xticklabels(class_names)\n  ax.set_yticklabels(class_names)\n  b, t = plt.ylim() # discover the values for bottom and top\n  b += 0.5 # Add 0.5 to the bottom\n  t -= 0.5 # Subtract 0.5 from the top\n  plt.ylim(b, t) # update the ylim(bottom, top) values\n  #plt.show() # ta-da!\n  plt.savefig(\"confusion_matrix_lstm.png\")\nplot_cm(enc.inverse_transform(y_test),enc.inverse_transform(y_pred),\n        ['D01', 'D02', 'D03', 'D04', 'D05','F01','F02','F03','F04','F05'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))\nplt.figure(dpi=1200)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\nplt.savefig('loss_val_loss_lstm.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nepochs = range(len(loss))\nplt.figure(dpi=1200)\nplt.plot(epochs, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\nplt.title('Training and validation Accuracy')\nplt.legend()\nplt.show()\nplt.savefig('acc_val_lstm.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Load TFLite model and allocate tensors.\n# interpreter = tf.lite.Interpreter(model_content=tflite_model)\n# interpreter.allocate_tensors()\n# input_details = interpreter.get_input_details()\n# output_details = interpreter.get_output_details()\n# interpreter.set_tensor(input_details[0]['index'], input_data)\n# interpreter.invoke()\n# # Get input and output tensors.\n\n# output_data = interpreter.get_tensor(output_details[0]['index'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from tensorflow import keras\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense,Flatten,LSTM,Dropout\n# from tensorflow.keras.regularizers import l2\n# from tensorflow.keras.optimizers import Adam\n# from keras.callbacks import ModelCheckpoint\n# import tensorflow as tf\n\n# verbose, epochs, batchSize,nHidden1,nHidden2 = 1, 80, 128,80,30\n\n# checkpoint_path = \"training_1/cp.ckpt\"\n# checkpoint_dir = os.path.dirname(checkpoint_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import keras\n# N_TIME_STEPS, N_FEATURES, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n# model = Sequential()\n# model.add(LSTM(nHidden1, input_shape=(N_TIME_STEPS,N_FEATURES)))\n# model.add(Dropout(0.7))\n# model.add(Dense(nHidden2, activation='tanh'))\n# model.add(Dense(n_outputs, activation='softmax'))\n# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# # fit network\n# # model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n# # evaluate model\n# # _, accuracy = model.evaluate(X_test, y_train, batch_size=batch_size, verbose=1)\n# # print(accuracy)\n\n\n# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n#                                                  save_weights_only=True,save_best_only=True,\n#                                                  verbose=1)\n\n# # callbacks= [ModelCheckpoint('model.h5', save_weights_only=False, save_best_only=True, verbose=1)]\n# history = model.fit(X_train,y_train,shuffle=False,epochs=epochs,verbose=verbose, validation_data=(X_val,y_val), callbacks=cp_callback)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _, accuracy = model.evaluate(X_test, y_test, verbose=1)\n# print(accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.models import load_model\n# model=load_model('./model.h5')\n# model.summary()\n# model.save(\"model\")\n# model=load_model('model')\n# loss, acc = model.evaluate(X_test, y_test, verbose=1)\n# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.DataFrame(history.history).plot(figsize=(25, 25))\n# plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report\n# from sklearn.metrics import plot_confusion_matrix\n# from sklearn.metrics import confusion_matrix\n# y_test_arg=np.argmax(y_test,axis=1)\n# Y_pred = np.argmax(model.predict(X_test),axis=1)\n# # predictions = model.predict(X_test)\n# # cm = confusion_matrix(y_test_arg, Y_pred)\n# # plot_confusion_matrix(cm, X_test,np.unique(Y_pred))\n# print(confusion_matrix(y_test_arg, Y_pred))\n# plot_confusion_matrix(cm, np.argmax(X_test,axis=1),np.unique(Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score, f1_score, confusion_matrix\ncm = confusion_matrix(enc.inverse_transform(y_test),enc.inverse_transform(y_pred))\n\ndef evaluate_metrics(confusion_matrix, y_test, y_pred, print_result=False, f1_avg='macro'):\n    # https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n    \n    TP = np.diag(confusion_matrix)\n    FP = confusion_matrix.sum(axis=0) - TP\n    FN = confusion_matrix.sum(axis=1) - TP    \n    TN = confusion_matrix.sum() - (FP + FN + TP)\n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP / (TP + FN)\n    # Specificity or true negative rate\n    TNR = TN / (TN + FP)\n    # Precision or positive predictive value\n    PPV = TP / (TP + FP)\n    # Negative predictive value\n    NPV = TN / (TN + FN)\n    # Fall out or false positive rate\n    FPR = FP / (FP + TN)\n    # False negative rate\n    FNR = FN / (TP + FN)\n    # False discovery rate\n    FDR = FP / (TP + FP)\n\n    # Overall accuracy\n    ACC = (TP + TN) / (TP + FP + FN + TN)\n    # ACC_micro = (sum(TP) + sum(TN)) / (sum(TP) + sum(FP) + sum(FN) + sum(TN))\n    ACC_macro = np.mean(\n        ACC)  # to get a sense of effectiveness of our method on the small classes we computed this average (macro-average)\n\n    f1 = f1_score(y_test, y_pred, average=f1_avg)\n    kappa = cohen_kappa_score(y_test, y_pred)\n    \n    if (print_result):\n        print(\"\\n\")\n        print(\"\\n\")\n        print(\"============ METRICS ============\")\n        print(confusion_matrix)\n        print(\"Accuracy (macro) : \", ACC_macro)        \n        print(\"F1 score         : \", f1)\n        print(\"Cohen Kappa score: \", kappa)\n        print(\"======= Per class metrics =======\")\n        print(\"Accuracy         : \", ACC)\n        print(\"Sensitivity (TPR): \", TPR)\n        print(\"Specificity (TNR): \", TNR)\n        print(\"Precision (+P)   : \", PPV)\n    \n    return ACC_macro, ACC, TPR, TNR, PPV, f1, kappa\n\n# acc=evaluate_metrics(cm, enc.inverse_transform(y_pred),enc.inverse_transform(y_test),True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}